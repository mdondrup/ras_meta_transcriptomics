# Main entrypoint of the workflow. 
# Please follow the best practices: 
# https://snakemake.readthedocs.io/en/stable/snakefiles/best_practices.html,
# in particular regarding the standardized folder structure mentioned there. 
import os
import re

WORKFLOW_NAME="RAS Meta-transcriptomics"

configfile: "config/config.yaml"

from snakemake.utils import min_version
min_version("8.14") 

FASTQ_DIR="input/fastq"
REF="input/reference"
REF_FASTA=config["reference_fasta"]
REF_GFF=config["reference_gff"]
FQ_SUF=config["fastq_file_suffix"]


include: "rules/setup.smk"


onsuccess:
    print( WORKFLOW_NAME+" finished, no error")
    shell("send_message.py '" + WORKFLOW_NAME + " finished, no error'") # comment out this line if you don't have a messenger script
onerror:
    print("An error occurred, see the log")
    shell("send_message.py 'An error occured in " + WORKFLOW_NAME + ", see the log: {log}'")
    
onstart:    
    print ("----------------------------------------------------------------------------")
    print ("Starting " + WORKFLOW_NAME)
    print ("Looking for fastq files in " + FASTQ_DIR)
    print ("Looking for a single reference fasta and reference annotation files in GFF3 format: " + REF_FASTA +" and "+ REF_GFF  )
    print ("If your meta-genome annotation in distributed over multiple files, you have to import them with rule 'import_reference'")

    print ("----------------------------------------------------------------------------")
    shell ('send_message.py "' + WORKFLOW_NAME + ' started at $(date)"') # comment out this line if you don't have a messenger script


# Function to get sample names by trimming suffixes from filenames
def get_samples():    
    samples = list(set(['_'.join(f.split('_')[0:2]) for f in os.listdir(FASTQ_DIR) if f.endswith(FQ_SUF)]))
    if not samples:
        raise FileNotFoundError("No FASTQ files found in the directory " + FASTQ_DIR + ".(Looking for suffix: " + FQ_SUF )
    return samples

# Function to get the R1 and R2 fastq files for paired-end samples
def get_illumina_fastqs(wildcards):
    r1 = [os.path.join(FASTQ_DIR, f) for f in os.listdir(FASTQ_DIR)
          if re.search('^' + wildcards.sample + ".*_1\\.fq\\.gz$", f)]
    r2 = [os.path.join(FASTQ_DIR, f) for f in os.listdir(FASTQ_DIR)
          if re.search('^' + wildcards.sample + ".*_2\\.fq\\.gz$", f)]
    if r1 and r2:
        return [r1[0], r2[0]]
    else:
        raise FileNotFoundError("No FASTQ files found for sample {wildcards.sample} or files not paired.")


    

    

# Getting the samples
SAMPLES = get_samples()
print("Found "+ str(len(SAMPLES)) +" samples:")
print(SAMPLES)    

    
rule all:
    input:
        ".setup_done",
        expand("fastp/{sample}_1.trimmed.fq.gz", sample=SAMPLES),
        expand("fastp/{sample}_2.trimmed.fq.gz", sample=SAMPLES),
        expand("fastp/{sample}.fastp_report.html", sample=SAMPLES),
        expand("fastp/{sample}.fastp_report.json", sample=SAMPLES),
        expand("sortmerna/{sample}_sortmerna_pe_stats.log", sample=SAMPLES),
        expand("aligned/{sample}.aln.sorted.bam", sample=SAMPLES),        
        "multiqc_report/multiqc_report.html",
        "results/allSamples.featureCounts",
        "results/de_analysis.html",
             

refdir=config["import_ref_dir"]
to_exclude=config["import_ref_exclude"] ## provide a list of reference directories to skip

# Function to get all fastq files in each fastq_pass directory
               


def get_reference_fastas(refdir):
    res = {'samples':[], 'fnas':[], 'gffs':[]}
    check = {}
    print ("Excluding directories: "+ str(to_exclude))
    for di in os.listdir(refdir):
        print("looking into directory "+di)
        if (di in to_exclude):
            print(" - excluding: "+ di + " from reference import" )
            continue
        if (os.path.isdir(os.path.join(refdir,di))):            
            for fi in os.listdir(os.path.join(refdir,di)) :
                if fi.endswith(".fna"):
                    res['samples'] += [di]
                    res['fnas'] += [fi]
                if fi.endswith('gff'):
                    res['gffs'] += [fi]                     
    return (res)
            
if(refdir):
    ref_fnas=get_reference_fastas(refdir)
#print (ref_fnas)

    

"""

### These rules are deliberately detached from the rest of the workflow because the contig renaming is
### very case-specific.
### The importer can merge multiple genome annotations into one.
### We assume here that the annotation files are bundled in a separate subdirectory of the import directory each,
### one directory per sample or MAG. The directory name determines the genome prefix.
### A single file with the extension .fna and gff are expected per genome/MAG

"""

rule import_reference:
    conda: "envs/agat.yaml"
    input: fnas=expand("input/reference/renamed-{sample}-contigs-{fna}", zip, sample=ref_fnas['samples'], fna=ref_fnas['fnas']),
           gffs=expand("input/reference/renamed-{sample}-annotation-{gff}", zip, sample=ref_fnas['samples'], gff=ref_fnas['gffs'])
    output: ".reference_imported" # [REF_FASTA, REF_GFF] ## This is left empty to prevent automatic invocation by the main WF
    log: "logs/agat_sp_merge_annotations.log"
    message: "Importing reference files {input}, renaming contigs and merging GFF files. See the log for potential problems: {log}" 
    shell:
        r"""
        cat {input.fnas} > input/reference/reference.fasta
        gffs=( {input.gffs} )
        gffs=( "${{gffs[@]/#/ --gff }}" )
	rm -f input/reference/reference.gff # agat won't overwrite
        agat_sp_merge_annotations.pl ${{gffs[@]}} --out input/reference/reference.gff > {log} 2>&1
        touch .reference_imported
        """


rule import_one_fasta:
    conda: "envs/seqkit.yaml"
    input: refdir+"/{sample}/{fna}"
    output:  "input/reference/renamed-{sample}-contigs-{fna}"
    message: "Importing {input} and prepending {wildcards.sample} to contig names"
    threads: 1         
    shell:
        r"""
        #echo Importing {output} and pre-pending {wildcards.sample} to contig names
        cat {input} | seqkit replace -p ^ -r {wildcards.sample}- > {output}           
        """

rule import_one_gff:
    input: refdir+"/{sample}/{gff}"
    output:  "input/reference/renamed-{sample}-annotation-{gff}"
    message: "Importing {input} and prepending {wildcards.sample} to landmark names. Feature IDs must be unique over all GFF files. FASTA sequences in GFF input will be removed!"
    shell:
        r"""       
        ## prepend the identifier to ##sequence-region tags
        ## if the gff file contains FASTA sequences (Following ##FASTA or '>') they will be ignored 
        awk '/^##FASTA/{{exit 0}}; /^>/{{exit 0}}; /##sequence-region/ {{ $2="{wildcards.sample}-"$2}}1' {input} | \
        ## prepend the identifier to each feature line
        awk  -v FS='\t' -v OFS='\t' '!/#/ {{ $1="{wildcards.sample}-"$1}}1' > {output}      
        """
REFS = config['metaquast_refs']
##############################################################################
# Rule: Checking the input assembly with metaQuast
rule metaquast:
    conda: "envs/metaquast.yaml",
    input:
        assm="input/reference/reference.fasta",
        refs=expand("ncbi_dataset/data/{ref}.fna", ref=REFS)
    output:
        directory("results/metaQUAST")
    params:
        refs = config['metaquast_refs'],
    threads: 60
    message: "Running MetaQUAST on the reference assembly"
    log: "logs/metaquast.log"         
    shell:
        r"""       
        REFS="{input.refs}"
        metaquast {input.assm} -r ${{REFS// /,}} -o {output} -t {threads} > {log} 2>&1    
        """
        
        


##############################################################################

# Rule: Adapter removal and quality trimming with fastp
rule fastp_pe:
    conda: "envs/fastp.yaml"
    input:
        lambda wildcards: get_illumina_fastqs(wildcards)
    output:
        r1="fastp/{sample}_1.trimmed.fq.gz",
        r2="fastp/{sample}_2.trimmed.fq.gz",
        html="fastp/{sample}.fastp_report.html",
        json="fastp/{sample}.fastp_report.json"
    threads: 16
    shell:
        r"""
        fastp -w {threads} -i {input[0]} -I {input[1]} -o {output.r1} -O {output.r2} \
        --qualified_quality_phred 20 --length_required 30  -j {output.json} -h {output.html}
        """
        
# Rule: Generate MultiQC report
rule multiqc:
    conda: "envs/multiqc.yaml"
    input:
        expand("fastp/{sample}.fastp_report.json", sample=SAMPLES),
        expand("sortmerna/{sample}_sortmerna_pe_stats.log", sample=SAMPLES),

        expand("aligned/{sample}.bamstats", sample=SAMPLES),
        "results/allSamples.featureCounts.summary",
       

     #   expand("aligned/{sample}.coverage.txt", sample=SAMPLES),
     #   expand("aligned/{sample}.genome_coverage.txt", sample=SAMPLES)
    output:
        "multiqc_report/multiqc_report.html"
    shell:
        """
        multiqc --force -o multiqc_report --ignore ".*" fastp/ sortmerna/ aligned/ results/

        """

rule sortmerna_pe:
    input:
        reads=["fastp/{sample}_1.trimmed.fq.gz", "fastp/{sample}_2.trimmed.fq.gz"],
        ref=["rRNA_databases_v4/smr_v4.3_default_db.fasta"]
    output:
        aligned=["sortmerna/{sample}_aligned_1.fq.gz", "sortmerna/{sample}_aligned_2.fq.gz"],
        other=["sortmerna/{sample}_filtered_1.fq.gz", "sortmerna/{sample}_filtered_2.fq.gz"],
        stats="sortmerna/{sample}_sortmerna_pe_stats.log",
    params:
        extra="--paired_in --out2 -blast 1 -num_alignments 1",
    threads: 10
    resources:
        mem_mb=10000,  # amount of memory for building the index
    log:
        "logs/sortmerna/{sample}_reads_pe.log"
    benchmark: "logs/sortmerna/{sample}_reads_pe.benchmark"
    message: "Running SortmeRNA on sample {wildcards.sample}"           
    wrapper:
        "v4.6.0/bio/sortmerna"

        




        
rule minimap2_index:
    input:
        target=REF_FASTA
    output:
        "{input1}.mmi"
    log:
        "logs/minimap2_index/{input1}.log"
    params:
        extra=""  # optional additional args
    threads: 20
    message: "Indexing reference sequence {input} for minimap2"         
    wrapper:
        "v4.6.0/bio/minimap2/index"        

        
rule minimap2_bam_sorted:
    input:
        target=REF_FASTA + ".mmi",  # can be either genome index or genome fasta
        query=["sortmerna/{sample}_filtered_1.fq.gz", "sortmerna/{sample}_filtered_2.fq.gz"],
    output:
        "aligned/{sample}.aln.sorted.bam",
    log:
        "logs/minimap2/{sample}.log"
    benchmark: "benchmarks/minimap2/{sample}.benchmark"
    params:
        extra="-ax sr",  # short-reads
        sorting="coordinate",  # optional: Enable sorting. Possible values: 'none', 'queryname' or 'coordinate'
        sort_extra="",  # optional: extra arguments for samtools/picard
    threads: 20
    message: "Running minimap2 on {wildcards.sample}"         
    wrapper:
        "v4.6.0/bio/minimap2/aligner"

rule bamtools_stats:
    input:
        "aligned/{sample}.aln.sorted.bam",
    output:
        "aligned/{sample}.bamstats"
    params:
        "-insert" # optional summarize insert size data
    log:
        "logs/bamtools/stats/{sample}.log"
    wrapper:
        "v4.6.0/bio/bamtools/stats"        
        


rule feature_counts_s2:
    input:
        # list of sam or bam files
        samples=expand("aligned/{sample}.aln.sorted.bam", sample=SAMPLES),
        annotation=REF_GFF,
        # optional input
        #chr_names="",           # implicitly sets the -A flag
        #fasta="genome.fasta"    # implicitly sets the -G flag
    output:
        multiext(
            "results/allSamples",
            ".featureCounts",
            ".featureCounts.summary"
        ),
    threads: 64
    params:
        strand=0,  # optional; strandness of the library (0: unstranded [default], 1: stranded, and 2: reversely stranded)
        r_path="",  # implicitly sets the --Rpath flag
        extra="-O -M --fraction -t gene -g locus_tag -s 2 -p",
    log:
        "logs/featureCounts.log",
    wrapper:
        "v4.6.0/bio/subread/featurecounts"

rule DE_analysis:
    conda: "envs/deanalysis.yaml"
    input:
        counts="results/allSamples.featureCounts",
        gff=REF_GFF,
        meta_data="input/meta_data.xlsx"
    output:
        "results/de_analysis.html",
    threads: 12
    params:
        min_counts=0,
        min_samples=0
    script:
        "scripts/de_analysis.rmd"       
